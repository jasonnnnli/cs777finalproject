# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qMxGVv0jQk2uKVJQA-OwYoh5DInVoiWs
"""

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Tokenizer, HashingTF, IDF
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.sql.functions import col, isnan, when, count
from pyspark.sql.types import IntegerType

# Initialize a Spark session
spark = SparkSession.builder.appName("CommentsPrediction").getOrCreate()

# Load the train and test data
train_data = spark.read.csv("/content/train.csv", header=True, inferSchema=True)
test_data = spark.read.csv("/content/test.csv", header=True, inferSchema=True)
train_data = train_data.withColumn("n_comments", col("n_comments").cast(IntegerType()))
# Convert 'word_count' to integer type and handle NaN values
def preprocess_data(df):
    df = df.withColumn("word_count", col("word_count").cast(IntegerType()))
    df = df.na.fill({"word_count": 0})  # Replace NaN with 0 in 'word_count'
    df = df.withColumn("word_count", when(isnan(col("word_count")), 0).otherwise(col("word_count")))

    # Handle NaN in other columns
    for column in df.columns:
        df = df.withColumn(column, when(col(column).isNull() | isnan(col(column)), 0).otherwise(col(column)))
    return df

train_data = preprocess_data(train_data)
test_data = preprocess_data(test_data)

# Data preprocessing steps
# Handling categorical variables
categorical_cols = ["newsdesk", "section", "subsection", "material"]
indexers = [
    StringIndexer(inputCol=column, outputCol=column+"_index", handleInvalid="keep").fit(train_data)
    for column in categorical_cols
]
encoders = [OneHotEncoder(inputCol=column+"_index", outputCol=column+"_vec") for column in categorical_cols]
from pyspark.sql.functions import col


# Handling text columns
text_cols = ["headline", "abstract", "keywords"]
for column in text_cols:
    train_data = train_data.na.fill({column: ''})
    test_data = test_data.na.fill({column: ''})

tokenizer = [Tokenizer(inputCol=column, outputCol=column+"_tokens") for column in text_cols]
hashingTF = [HashingTF(inputCol=column+"_tokens", outputCol=column+"_tf", numFeatures=1000) for column in text_cols]
idf = [IDF(inputCol=column+"_tf", outputCol=column+"_features") for column in text_cols]

# Vector assembler to combine feature columns
assembler_inputs = [c+"_vec" for c in categorical_cols] + [t+"_features" for t in text_cols] + ["word_count"]
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features", handleInvalid="skip")

# Define the model
rf = RandomForestRegressor(featuresCol="features", labelCol="n_comments")

# Define the pipeline
pipeline = Pipeline(stages=indexers + encoders + tokenizer + hashingTF + idf + [assembler, rf])

# Train the model
model = pipeline.fit(train_data)

# Predictions
predictions = model.transform(test_data)

# Display first few predictions
predictions.select("features", "prediction").show(5)
# Make predictions on the training data
train_predictions = model.transform(train_data)

# Instantiate the evaluator
evaluator_rmse = RegressionEvaluator(labelCol="n_comments", predictionCol="prediction", metricName="rmse")
evaluator_mae = RegressionEvaluator(labelCol="n_comments", predictionCol="prediction", metricName="mae")
evaluator_r2 = RegressionEvaluator(labelCol="n_comments", predictionCol="prediction", metricName="r2")

# Compute metrics on the training data
rmse = evaluator_rmse.evaluate(train_predictions)
mae = evaluator_mae.evaluate(train_predictions)
r2 = evaluator_r2.evaluate(train_predictions)

print(f"Root Mean Squared Error (RMSE) on Training Data: {rmse}")
print(f"Mean Absolute Error (MAE) on Training Data: {mae}")
print(f"R-Squared (R2) on Training Data: {r2}")

# Stop the Spark session
spark.stop()

!pip install pyspark

